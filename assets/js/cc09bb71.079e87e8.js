"use strict";(self.webpackChunkwiki_loliot_net=self.webpackChunkwiki_loliot_net||[]).push([[7696],{11470:(e,n,t)=>{t.d(n,{A:()=>j});var a=t(96540),o=t(34164),i=t(23104),r=t(56347),s=t(205),l=t(57485),c=t(31682),d=t(70679);function u(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:o}}=e;return{value:n,label:t,attributes:a,default:o}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function p(e){let{queryString:n=!1,groupId:t}=e;const o=(0,r.W6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(i),(0,a.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(o.location.search);n.set(i,e),o.replace({...o.location,search:n.toString()})}),[i,o])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,i=h(e),[r,l]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:i}))),[c,u]=p({queryString:t,groupId:o}),[g,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[o,i]=(0,d.Dv)(t);return[o,(0,a.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:o}),_=(()=>{const e=c??g;return m({value:e,tabValues:i})?e:null})();(0,s.A)((()=>{_&&l(_)}),[_]);return{selectedValue:r,selectValue:(0,a.useCallback)((e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),f(e)}),[u,f,i]),tabValues:i}}var f=t(92303);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function x(e){let{className:n,block:t,selectedValue:a,selectValue:r,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),d=e=>{const n=e.currentTarget,t=l.indexOf(n),o=s[t].value;o!==a&&(c(n),r(o))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":t},n),children:s.map((e=>{let{value:n,label:t,attributes:i}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>{l.push(e)},onKeyDown:u,onClick:d,...i,className:(0,o.A)("tabs__item",_.tabItem,i?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function v(e){let{lazy:n,children:t,selectedValue:i}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function y(e){const n=g(e);return(0,b.jsxs)("div",{className:(0,o.A)("tabs-container",_.tabList),children:[(0,b.jsx)(x,{...n,...e}),(0,b.jsx)(v,{...n,...e})]})}function j(e){const n=(0,f.A)();return(0,b.jsx)(y,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(34164);const o={tabItem:"tabItem_Ymn6"};var i=t(74848);function r(e){let{children:n,hidden:t,className:r}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,a.A)(o.tabItem,r),hidden:t,children:n})}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var a=t(96540);const o={},i=a.createContext(o);function r(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(i.Provider,{value:n},e.children)}},64275:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"lang/python/libraries/yolov4/python-yolov4-dataset","title":"Dataset for YOLOv4","description":"Dataset for YOLOv4","source":"@site/docs/lang/python/libraries/yolov4/python-yolov4-dataset.mdx","sourceDirName":"lang/python/libraries/yolov4","slug":"/lang/python/libraries/yolov4/python-yolov4-dataset","permalink":"/docs/lang/python/libraries/yolov4/python-yolov4-dataset","draft":false,"unlisted":false,"editUrl":"https://github.com/hhk7734/wiki/tree/main/docs/lang/python/libraries/yolov4/python-yolov4-dataset.mdx","tags":[],"version":"current","lastUpdatedAt":1744914495000,"frontMatter":{"id":"python-yolov4-dataset","title":"Dataset for YOLOv4","sidebar_label":"Dataset","description":"Dataset for YOLOv4","keywords":["Neural Network","YOLOv4","dataset"]},"sidebar":"python","previous":{"title":"About","permalink":"/docs/lang/python/libraries/yolov4/python-yolov4-about"},"next":{"title":"Training","permalink":"/docs/lang/python/libraries/yolov4/python-yolov4-training"}}');var o=t(74848),i=t(28453),r=t(11470),s=t(19365);const l={id:"python-yolov4-dataset",title:"Dataset for YOLOv4",sidebar_label:"Dataset",description:"Dataset for YOLOv4",keywords:["Neural Network","YOLOv4","dataset"]},c=void 0,d={},u=[{value:"Dataset files and formats",id:"dataset-files-and-formats",level:2},{value:"converted-coco",id:"converted-coco",level:3},{value:"yolo",id:"yolo",level:3},{value:"Convert coco to custom dataset",id:"convert-coco-to-custom-dataset",level:2},{value:"COCO 2017 Dataset",id:"coco-2017-dataset",level:3},{value:"Create names file",id:"create-names-file",level:3},{value:"Conversion script",id:"conversion-script",level:3},{value:"Dataset test script",id:"dataset-test-script",level:3}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"dataset-files-and-formats",children:"Dataset files and formats"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"class_id"})," is an integer greater than or equal to 0."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"center_x"}),", ",(0,o.jsx)(n.code,{children:"center_y"}),", ",(0,o.jsx)(n.code,{children:"width"}),"and ",(0,o.jsx)(n.code,{children:"height"})," are between 0.0 and 1.0."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"converted-coco",children:"converted-coco"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",metastring:'title="image_path_and_bboxes.txt"',children:"<relative or full path>/image_0.jpg <class_id>,<center_x>,<center_y>,<width>,<height> <class_id>,<center_x>,<center_y>,<width>,<height> ...\n<relative or full path>/image_1.jpg <class_id>,<center_x>,<center_y>,<width>,<height> ...\n<relative or full path>/image_2.jpg <class_id>,<center_x>,<center_y>,<width>,<height> ...\n\n...\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:"<full path>\n\u251c\u2500\u2500 image_0.jpg\n\u251c\u2500\u2500 image_1.jpg\n\u251c\u2500\u2500 image_2.jpg\n\u2514\u2500\u2500 ...\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Ref: ",(0,o.jsx)(n.a,{href:"https://github.com/hhk7734/tensorflow-yolov4/tree/master/test/dataset",children:"https://github.com/hhk7734/tensorflow-yolov4/tree/master/test/dataset"})]}),"\n",(0,o.jsx)(n.h3,{id:"yolo",children:"yolo"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",metastring:'title="image_path.txt"',children:"<relative or full path>/image_0.jpg\n<relative or full path>/image_1.jpg\n<relative or full path>/image_2.jpg\n\n...\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",metastring:'title="<image_name>.txt"',children:"<class_id> <center_x> <center_y> <width> <height>\n<class_id> <center_x> <center_y> <width> <height>\n<class_id> <center_x> <center_y> <width> <height>\n\n...\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:"<full path>\n\u251c\u2500\u2500 image_0.jpg\n\u251c\u2500\u2500 image_0.txt\n\u251c\u2500\u2500 image_1.jpg\n\u251c\u2500\u2500 image_1.txt\n\u251c\u2500\u2500 image_2.jpg\n\u251c\u2500\u2500 image_2.txt\n\u2514\u2500\u2500 ...\n"})}),"\n",(0,o.jsx)(n.h2,{id:"convert-coco-to-custom-dataset",children:"Convert coco to custom dataset"}),"\n",(0,o.jsx)(n.h3,{id:"coco-2017-dataset",children:"COCO 2017 Dataset"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n    "info": {\n        "description": "COCO 2017 Dataset",\n        "url": "http://cocodataset.org",\n        "version": "1.0",\n        "year": 2017,\n        "contributor": "COCO Consortium",\n        "date_created": "2017/09/01"\n    },\n    "licenses": [\n        {\n            "url": "http://creativecommons.org/licenses/by-nc-sa/2.0/",\n            "id": 1,\n            "name": "Attribution-NonCommercial-ShareAlike License"\n        },\n        ...\n    ],\n    "images": [\n        {\n            "license": 4,\n            "file_name": "000000397133.jpg",\n            "coco_url": "http://images.cocodataset.org/val2017/000000397133.jpg",\n            "height": 427,\n            "width": 640,\n            "date_captured": "2013-11-14 17:02:52",\n            "flickr_url": "http://farm7.staticflickr.com/6116/6255196340_da26cf2c9e_z.jpg",\n            "id": 397133\n        },\n        ...\n    ],\n    "annotations": [\n        {\n            "segmentation": [[...]],\n            "area": 702.1057499999998,\n            "iscrowd": 0,\n            "image_id": 289343,\n            "bbox": [473.07,395.93,38.65,28.67], // xmin, ymin, width, height\n            "category_id": 18,\n            "id": 1768\n        },\n        ...\n    ],\n    "categories": [\n        {\n            "supercategory": "person",\n            "id": 1,\n            "name": "person"\n        },\n        {\n            "supercategory": "vehicle",\n            "id": 2,\n            "name": "bicycle"\n        },\n        ...\n    ]\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"create-names-file",children:"Create names file"}),"\n",(0,o.jsxs)(n.p,{children:["Create a file with names you want to predict. The class name should be included in ",(0,o.jsx)(n.code,{children:"categories"})," above."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",metastring:'title="custom.names"',children:"person\nbicycle\ncar\nmotorbike\naeroplane\nbus\n"})}),"\n",(0,o.jsx)(n.h3,{id:"conversion-script",children:"Conversion script"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"2020-10-16"}),"\n",(0,o.jsx)(n.li,{children:"OS: Ubuntu 20.04"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import json\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\nINSTANCES_PATH = "instances_train2017.json"\nNAMES_PATH = "custom.names"\nOUTPUT_FILE_PATH = "custom_train2017.txt"\n\ncoco = json.load(open(INSTANCES_PATH))\nimages = coco["images"]\nannotations = coco["annotations"]\ncategories = coco["categories"]\nreplaced_name = {\n    "couch": "sofa",\n    "airplane": "aeroplane",\n    "tv": "tvmonitor",\n    "motorcycle": "motorbike",\n}\n\nclass_to_id = {}\nid_to_class = {}\nwith open(NAMES_PATH, "r") as fd:\n    index = 0\n    for class_name in fd:\n        class_name = class_name.strip()\n        if len(class_name) != 0:\n            id_to_class[index] = class_name\n            class_to_id[class_name] = index\n            index += 1\n\ndataset = {}\n\nfor annotation in tqdm(annotations, desc="Parsing"):\n    image_id = annotation["image_id"]\n    category_id = annotation["category_id"]\n\n    # Find image\n    file_name = None\n    image_height = 0\n    image_width = 0\n    for image in images:\n        if image["id"] == image_id:\n            file_name = image["file_name"]\n            image_height = image["height"]\n            image_width = image["width"]\n            break\n\n    if file_name is None:\n        continue\n\n    # Find class id\n    class_id = None\n    for category in categories:\n        if category["id"] == category_id:\n            category_name = category["name"]\n            if category_name in replaced_name:\n                category_name = replaced_name[category_name]\n\n            class_id = class_to_id.get(category_name)\n            break\n\n    if class_id is None:\n        continue\n\n    # Calculate x,y,w,h\n    x_center = annotation["bbox"][0] + annotation["bbox"][2] / 2\n    x_center /= image_width\n    y_center = annotation["bbox"][1] + annotation["bbox"][3] / 2\n    y_center /= image_height\n    width = annotation["bbox"][2] / image_width\n    height = annotation["bbox"][3] / image_height\n\n    if dataset.get(image_id):\n        dataset[image_id][1].append(\n            [class_id, x_center, y_center, width, height]\n        )\n    else:\n        dataset[image_id] = [\n            file_name,\n            [[class_id, x_center, y_center, width, height]],\n        ]\n\ndataset = OrderedDict(sorted(dataset.items()))\n\nwith open(OUTPUT_FILE_PATH, "w") as fd:\n    for image_id, bboxes in tqdm(dataset.items(), desc="Saving"):\n        data = bboxes[0]\n        for bbox in bboxes[1]:\n            data += " "\n            data += "{:d},".format(bbox[0])\n            data += "{:8.6f},".format(bbox[1])\n            data += "{:8.6f},".format(bbox[2])\n            data += "{:8.6f},".format(bbox[3])\n            data += "{:8.6f}".format(bbox[4])\n\n        data += "\\n"\n        fd.write(data)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"dataset-test-script",children:"Dataset test script"}),"\n",(0,o.jsxs)(r.A,{groupId:"yolov4-version",defaultValue:"v3",values:[{label:"v3",value:"v3"},{label:"v2",value:"v2"}],children:[(0,o.jsxs)(s.A,{value:"v3",children:[(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"2021-02-21"}),"\n",(0,o.jsx)(n.li,{children:"OS: Ubuntu 20.04"}),"\n",(0,o.jsx)(n.li,{children:"yolov4: v3.1.0"}),"\n"]}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom yolov4.tf import YOLOv4, YOLODataset\n\nyolo = YOLOv4()\nyolo.config.parse_names("test/coco.names")\nyolo.config.parse_cfg("config/yolov4-tiny.cfg")\n\ndataset = YOLODataset(\n    config=yolo.config,\n    dataset_list="/home/hhk7734/NN/val2017.txt",\n    image_path_prefix="/home/hhk7734/NN/val2017",\n)\n\ncount = 0\nfor i, (images, gt) in enumerate(dataset):\n    cv2.namedWindow("truth", cv2.WINDOW_AUTOSIZE)\n\n    beta_nms = yolo.config.metayolos[0].beta_nms\n    classes = yolo.config.metayolos[0].classes\n    stride = classes + 5\n    num_mask = len(yolo.config.masks[0])\n\n    candidates = []\n    for y, metayolo in enumerate(yolo.config.metayolos):\n        candidates.append(gt[y][..., : -len(yolo.config.yolo_0.mask)])\n\n    for batch in range(len(images)):\n        frame = images[batch, ...] * 255\n        frame = cv2.cvtColor(frame.astype(np.uint8), cv2.COLOR_RGB2BGR)\n        height, width, _ = frame.shape\n\n        yolos = []\n        for c, cand in enumerate(candidates):\n            truth = cand[batch : batch + 1, ...]\n            _, h, w, ch = truth.shape\n            mask = yolo.config.masks[c]\n            for n in range(num_mask):\n                xy_index = n * stride\n                wh_index = xy_index + 2\n                oc_index = xy_index + 4\n\n                xy = truth[..., xy_index:wh_index] * np.array([w, h])\n                truth[..., xy_index:wh_index] = xy - xy.astype(np.int)\n\n                awh = yolo.config.anchors[mask[n]]\n                wh = truth[..., wh_index:oc_index] / awh\n                truth[..., wh_index:oc_index] = np.log(\n                    wh + np.finfo(np.float32).eps\n                )\n\n        treu_bboxes = yolo.get_yolo_detections(\n            [c[batch : batch + 1, ...] for c in candidates]\n        )\n        # yolo.fit_to_original(treu_bboxes, height, width)\n        image2 = yolo.draw_bboxes(frame, treu_bboxes)\n        cv2.imshow("truth", image2)\n        while cv2.waitKey(10) & 0xFF != ord("q"):\n            pass\n        count += 1\n        if count == 30:\n            break\n    if count == 30:\n        break\n'})})]}),(0,o.jsxs)(s.A,{value:"v2",children:[(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"2020-10-27"}),"\n",(0,o.jsx)(n.li,{children:"OS: Ubuntu 20.04"}),"\n",(0,o.jsx)(n.li,{children:"yolov4: v2.0.0"}),"\n"]}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nimport tensorflow as tf\nfrom yolov4.tf import YOLOv4\n\nyolo = YOLOv4()\nyolo.classes = "coco.names"\nyolo.input_size = (640, 480)\nyolo.batch_size = 2\ndataset = yolo.load_dataset("train2017.txt", image_path_prefix="train2017")\n\nfor i, (images, gt) in enumerate(dataset):\n    for j in range(len(images)):\n        _candidates = []\n        for candidate in gt:\n            grid_size = candidate.shape[1:3]\n            _candidates.append(\n                tf.reshape(\n                    candidate[j], shape=(1, grid_size[0] * grid_size[1] * 3, -1)\n                )\n            )\n        candidates = np.concatenate(_candidates, axis=1)\n\n        frame = images[j, ...] * 255\n        frame = frame.astype(np.uint8)\n\n        pred_bboxes = yolo.candidates_to_pred_bboxes(candidates[0])\n        pred_bboxes = yolo.fit_pred_bboxes_to_original(pred_bboxes, frame.shape)\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n        image = yolo.draw_bboxes(frame, pred_bboxes)\n        cv2.namedWindow("result", cv2.WINDOW_AUTOSIZE)\n        cv2.imshow("result", image)\n        while cv2.waitKey(10) & 0xFF != ord("q"):\n            pass\n    if i == 10:\n        break\n\ncv2.destroyWindow("result")\n'})})]})]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}}}]);