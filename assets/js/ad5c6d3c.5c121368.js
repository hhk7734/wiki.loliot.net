"use strict";(self.webpackChunkwiki_loliot_net=self.webpackChunkwiki_loliot_net||[]).push([[40212],{16637:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"mlops/nn/nn-cpu-gpu-npu","title":"Neural Network CPU vs. GPU vs. NPU","description":"Neural Network CPU vs. GPU vs. NPU","source":"@site/docs/mlops/nn/nn-cpu-gpu-npu.mdx","sourceDirName":"mlops/nn","slug":"/mlops/nn/nn-cpu-gpu-npu","permalink":"/docs/mlops/nn/nn-cpu-gpu-npu","draft":false,"unlisted":false,"editUrl":"https://github.com/hhk7734/wiki/tree/main/docs/mlops/nn/nn-cpu-gpu-npu.mdx","tags":[],"version":"current","lastUpdatedAt":1735587312000,"frontMatter":{"id":"nn-cpu-gpu-npu","title":"Neural Network CPU vs. GPU vs. NPU","sidebar_label":"CPU vs. GPU vs. NPU","description":"Neural Network CPU vs. GPU vs. NPU","keywords":["Neural Network","CPU","GPU","NPU"]},"sidebar":"nn","previous":{"title":"Derivative","permalink":"/docs/mlops/nn/nn/derivative"},"next":{"title":"Conv2D","permalink":"/docs/mlops/nn/cnn/conv2d"}}');var r=s(74848),t=s(28453),i=s(24763);const a={id:"nn-cpu-gpu-npu",title:"Neural Network CPU vs. GPU vs. NPU",sidebar_label:"CPU vs. GPU vs. NPU",description:"Neural Network CPU vs. GPU vs. NPU",keywords:["Neural Network","CPU","GPU","NPU"]},l=void 0,c={},d=[{value:"Google&#39;s product News",id:"googles-product-news",level:2},{value:"How a CPU works",id:"how-a-cpu-works",level:3},{value:"How a GPU works",id:"how-a-gpu-works",level:3},{value:"How a TPU works",id:"how-a-tpu-works",level:3},{value:"CHA Diagram",id:"cha-diagram",level:2},{value:"SoC",id:"soc",level:3},{value:"CNS-core",id:"cns-core",level:3},{value:"N-core",id:"n-core",level:3},{value:"Reference",id:"reference",level:2}];function u(e){const n={a:"a",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"googles-product-news",children:"Google's product News"}),"\n",(0,r.jsx)(n.h3,{id:"how-a-cpu-works",children:"How a CPU works"}),"\n",(0,r.jsxs)(n.p,{children:["...\nA CPU has to store the calculation results on memory inside CPU (so called registers or L1 cache) for every single calculation. This memory access becomes the downside of CPU architecture called the ",(0,r.jsx)(n.strong,{children:"von Neumann bottleneck"}),".\n..."]}),"\n",(0,r.jsx)(n.h3,{id:"how-a-gpu-works",children:"How a GPU works"}),"\n",(0,r.jsx)(n.p,{children:"...\nThe modern GPU usually has 2,500\u20135,000 ALUs in a single processor that means you could execute thousands of multiplications and additions simultaneously. For every single calculation in the thousands of ALUs, GPU need to access registers or shared memory to read and store the intermediate calculation results.\n..."}),"\n",(0,r.jsx)(n.h3,{id:"how-a-tpu-works",children:"How a TPU works"}),"\n",(0,r.jsxs)(n.p,{children:["...\nThe key enabler is a major reduction of the von Neumann bottleneck. So hardware designer place thousands of multipliers and adders and connect them to each other directly to form a large physical matrix of those operators. This is called ",(0,r.jsx)(n.strong,{children:"systolic array"})," architecture."]}),"\n",(0,r.jsx)(n.p,{children:"Let's see how a systolic array executes the neural network calculations. At first, TPU loads the parameters from memory into the matrix of multipliers and adders."}),"\n",(0,r.jsxs)(n.p,{children:["Then, the TPU loads data from memory. As each multiplication is executed, the result will be passed to next multipliers while taking summation at the same time. So the output will be the summation of all multiplication result between data and parameters. During the whole process of massive calculations and data passing, ",(0,r.jsx)(n.strong,{children:"no memory access is required at all"}),".\n..."]}),"\n",(0,r.jsx)(n.h2,{id:"cha-diagram",children:"CHA Diagram"}),"\n",(0,r.jsx)(n.h3,{id:"soc",children:"SoC"}),"\n",(0,r.jsx)("center",{children:(0,r.jsx)("img",{src:(0,i.Ay)("img/mlops/nn/nn-cha-soc.jpg")})}),"\n",(0,r.jsx)(n.h3,{id:"cns-core",children:"CNS-core"}),"\n",(0,r.jsx)("center",{children:(0,r.jsx)("img",{src:(0,i.Ay)("img/mlops/nn/nn-cha-cns.jpg")})}),"\n",(0,r.jsx)(n.h3,{id:"n-core",children:"N-core"}),"\n",(0,r.jsx)("center",{children:(0,r.jsx)("img",{src:(0,i.Ay)("img/mlops/nn/nn-cha-ncore.jpg")})}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning",children:"https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://en.wikichip.org/wiki/centaur/microarchitectures/cha",children:"https://en.wikichip.org/wiki/centaur/microarchitectures/cha"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>a});var o=s(96540);const r={},t=o.createContext(r);function i(e){const n=o.useContext(t);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);